>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 23      |
|-----------------|
| Maliha Chowdhury Adrita                |   
| Wahid Chowdhury              |   
| Himel Paul               |   
| M Munem Morshed                |   
| Kosy Onyejemezi                |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

Before this lab, our understanding of exploratory and manual testing was mostly theoretical. We knew they were different but related approaches—exploratory testing being more flexible and experience-driven, while manual scripted testing followed predefined test cases to check if the system met its requirements.

Through hands-on experience, we tested a Java-based ATM system using exploratory, manual scripted, and regression testing. We reported bugs using Jira and compared different testing methods to see which was more effective. By the end of the lab, we gained a clearer understanding of how these approaches work together to improve software reliability.

# High-level description of the exploratory testing plan

Our goal was to test the ATM system thoroughly and identify any issues that might not be caught through predefined test cases. We started by reviewing Appendix B, which provided details on how the system was supposed to function. This helped us understand its expected behavior and plan our testing accordingly.

To stay organized, we divided the ATM’s features into key areas: login and security, withdrawals, deposits, transfers, and balance inquiry. Each area was tested to check for common issues like incorrect PIN attempts, invalid withdrawals, incorrect balance updates, and unexpected errors during transfers.

Two testers were assigned to exploratory testing, each using a ATM card to simulate real user interactions. Since time was limited, we set a 30-minute window for testing, ensuring each category received equal attention. The testers explored the system freely, trying different inputs and actions while observing for unexpected behaviors. Any issues discovered were documented in Jira with clear descriptions and reproduction steps. This approach allowed us to test real-world scenarios while maintaining flexibility to uncover hidden defects.

# Comparison of exploratory and manual functional testing

Exploratory and manual functional testing both help ensure software quality, but they take different approaches. Exploratory testing is more flexible, allowing testers to interact with the system freely and uncover unexpected issues. Since there are no predefined test cases, testers rely on their intuition and experience to find problems that might not be covered in a structured test plan. This makes exploratory testing useful for discovering hidden bugs, but it can also be inconsistent because different testers may focus on different areas.

Manual functional testing, on the other hand, follows a structured approach. Testers use predefined test cases to check if the system meets its requirements. This makes it easier to track results and ensure that testing is consistent across different versions. However, because manual testing sticks to a set script, it might miss issues that only show up in real-world usage.

Both methods have their advantages. Exploratory testing allows testers to think outside the box and find unexpected bugs, while manual functional testing ensures thorough, repeatable checks. Together, they provide a balanced way to test software and make sure it works as expected

# Notes and discussion of the peer reviews of defect reports

After completing the testing, defect reports were reviewed to ensure they were clear, accurate, and easy to follow. Some defects were found by more than one tester, confirming their validity. In some cases, reports needed better descriptions or more precise steps to reproduce the issues. Suggestions were made to improve clarity and organization to make the reports more effective.

Defects were also categorized based on severity, making it easier to prioritize fixes. Some issues affected critical functions like withdrawals and deposits, while others were less significant but still worth noting.

The peer review process helped improve the overall quality of the defect reports and provided valuable insights into how different testing approaches can reveal various issues. It also reinforced the importance of well-documented defect reports in software testing.

# How the pair testing was managed and team work/effort was divided 

To ensure thorough testing, the team divided tasks based on different testing methods. Two members focused on exploratory testing, freely navigating the system to uncover unexpected issues. They tested various functionalities without predefined test cases, observing how the system behaved under different conditions.

The other three members conducted manual functional testing, following predefined test cases to systematically verify whether the ATM system met its expected requirements. They ensured that each function worked correctly and consistently, comparing actual results with expected outcomes.

Once the initial testing phases were completed, the entire team worked together on regression testing. This involved retesting previously reported defects on an updated version of the system to confirm whether the issues had been fixed.

This division of work allowed for a balanced approach, combining both structured and flexible testing methods while ensuring that all team members contributed to the overall testing process

# Difficulties encountered, challenges overcome, and lessons learned

One of the biggest challenges was learning how to use the bug tracking system. At first, it was confusing to log defects properly and categorize them. To overcome this, we spent time exploring its features and helping each other figure it out. With practice, it became much easier to use.

Another difficulty was coordinating tasks since two of us were doing exploratory testing while the rest focused on manual testing. Making sure everyone stayed on the same page took some effort, but regular discussions helped keep things organized.

Time management was also a challenge. Balancing different tasks while making sure testing was thorough required us to stay focused. Setting clear goals and deadlines helped us manage our time better.

Through this experience, we learned that clear communication and documentation are key in software testing. Writing detailed defect reports made it easier for everyone to understand the issues. We also realized that both exploratory and manual testing are important—one helps find unexpected issues, while the other ensures everything works as expected.

Finally, working together on regression testing showed us how valuable teamwork is. By combining our efforts, we made sure defects were properly checked, improving the overall quality of our testing.

# Comments/feedback on the lab and lab document itself

The lab was a great learning experience, and everything went smoothly. It gave us hands-on practice with exploratory, manual, and regression testing, helping us understand how different testing methods work together to improve software quality. Using the bug tracking system was a new challenge, but we quickly got the hang of it.

The lab instructions were clear, and the submission process was straightforward. Overall, it was a well-structured lab that provided valuable real-world experience in software testing.
